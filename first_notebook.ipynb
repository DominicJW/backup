{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af21fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display,clear_output,Javascript\n",
    "import sys,random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d3e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 08:26:43.681843: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-12 08:26:44.069343: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-12 08:26:45.211209: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc45f4c03f4400d8e89a960957015c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "# model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "#     llm_int4_enable_fp32_cpu_offload = True, #i think the cpu offloading of gpu with quantization is incompatable, despite it saying\n",
    "    bnb_4bit_quant_type = \"nf4\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=quant_config,\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "#why did it need internet connection to load mistral?\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70084463",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2a37a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you help me with a question?\"},\n",
    "]\n",
    "\n",
    "\n",
    "#end of sequence token is added to end of assistants output, but not user's.\n",
    "\n",
    "tokenised_context = tokenizer.apply_chat_template(chat, tokenize=True,return_tensors=\"pt\").to(model.device)\n",
    "# tokenised_context = tokenizer(\"Sure thing, here are the lyrics to our song. Ver.1: \",return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "# model.set_attn_implementation('eager')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokenised_context,use_cache=True,output_attentions=True)\n",
    "\n",
    "past = outputs.past_key_values\n",
    "new_past1 = type(outputs.past_key_values)([layer for layer in outputs.past_key_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f239b3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 11, 128])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "past[0][0].shape\n",
    "#batch,num_layers,tokens,tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca93c4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 11, 128])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_past1[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a923392",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat2 = \"Yes of course!\"\n",
    "tokenised_context = tokenizer(chat2,return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d392379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs2 = model(input_ids=tokenised_context,use_cache=True,past_key_values = past,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "011eb3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 16, 128])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "72856fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 11, 128])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_past1[0][0].shape\n",
    "#need to make new past then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da08a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#128 is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10954199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.past_key_values[0])\n",
    "\n",
    "#32 layers\n",
    "#2 for(key and value)\n",
    "#1 for batch size\n",
    "#8 for the number of heads\n",
    "#16 for number of tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f08c0b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 5, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8d7edb31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.1951e-03, 1.2878e-02, 1.0315e-02, 8.0566e-02, 1.6928e-05, 2.1935e-05,\n",
       "         1.2457e-05, 1.2589e-03, 8.1635e-04, 4.5471e-03, 1.9238e-01, 3.9551e-02,\n",
       "         2.1553e-04, 2.7084e-04, 7.2327e-03, 6.7234e-05, 5.7861e-02, 9.3384e-03,\n",
       "         4.0771e-02, 3.5156e-02, 4.8828e-04, 1.5259e-05, 5.0354e-03, 2.0504e-04,\n",
       "         2.6123e-02, 2.4902e-02, 4.9316e-02, 1.9653e-02, 1.9409e-02, 1.8945e-01,\n",
       "         3.7354e-02, 9.1797e-02]], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.attentions[0][:,:,4,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "977ec158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "padded_tensor = F.pad(outputs.attentions[0], (0, 5, 0, 0), mode='constant', value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa4ac4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96d8741e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 11, 11])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.attentions[0].shape\n",
    "#layer, (as tuple dim)\n",
    "#batch\n",
    "#Q heads\n",
    "#new toks\n",
    "#total toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e05f80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 16, 16])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((outputs2.attentions[0],padded_tensor),dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6100d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = outputs2.attentions[0].shape[2]\n",
    "old_p_new = outputs2.attentions[0].shape[3]\n",
    "\n",
    "old = old_p_new - new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1189df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2.attentions[0][:,:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f18568d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1, 12])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.attentions[0][:,:,0:1,:12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a4b132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2958fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_logits = outputs.logits[0, -1, :]  # (vocab_size,)\n",
    "probs = F.softmax(last_logits, dim=-1)  # (vocab_size,)\n",
    "topk = torch.topk(probs, k=8)\n",
    "top_ids = topk.indices.tolist()\n",
    "top_probs = topk.values.tolist()\n",
    "toks = tokenizer.decode(top_ids)\n",
    "tok_to_prob = dict(zip(toks,top_probs))\n",
    "tokprob_to_tok = {f\"{key} {tok_to_prob[key]}\" : key for key in tok_to_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a87a7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dropdown_options(selected_token):\n",
    "    global past\n",
    "    if len(selected_token.split()) ==1:\n",
    "        tokid = torch.tensor(tokenizer.convert_tokens_to_ids(selected_token.split())).unsqueeze(0).to(model.device)\n",
    "    else:\n",
    "        tokid = tokenizer(selected_token,add_special_tokens=False,return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids = tokid,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past\n",
    "                   )\n",
    "    past = out.past_key_values\n",
    "    last_logits = out.logits[0, -1, :]  # (vocab_size,)\n",
    "    probs = F.softmax(last_logits, dim=-1)  # (vocab_size,)\n",
    "    topk = torch.topk(probs, k=8)\n",
    "    \n",
    "    top_ids = topk.indices.tolist()\n",
    "    top_probs = topk.values.tolist()\n",
    "    toks = tokenizer.convert_ids_to_tokens(top_ids) \n",
    "    return dict(zip(toks,top_probs))\n",
    "\n",
    "\n",
    "def handle_click(click):\n",
    "    global description_string  # Use a global variable to store the concatenated descriptions\n",
    "    global tokenised_context\n",
    "    selected_token = dropdown.value\n",
    "#     print(f'\\r{tokenizer.decode(tokenised_context.squeeze(0))} ...')#takes so long to get here\n",
    "    \n",
    "    tok_to_prob = generate_dropdown_options(selected_token)\n",
    "    tokprob_to_tok = {f\"{key.replace('Ġ', '').replace('Ċ', '')} {tok_to_prob[key]}\" : key for key in tok_to_prob}\n",
    "    \n",
    "    tokid = torch.tensor(tokenizer.convert_tokens_to_ids(selected_token)).unsqueeze(0).unsqueeze(0).to(model.device)\n",
    "    tokenised_context = torch.cat((tokenised_context,tokid),dim=-1)\n",
    "\n",
    "    dropdown.options = tokprob_to_tok\n",
    "\n",
    "\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(f'{tokenizer.decode(tokenised_context.squeeze(0))}')\n",
    "\n",
    "def handle_text_input(click):\n",
    "    global tokenised_context\n",
    "    input_text = text_box.value\n",
    "    tok_to_prob = generate_dropdown_options(input_text)\n",
    "    tokprob_to_tok = {f\"{key.replace('Ġ', '').replace('Ċ', '')} {tok_to_prob[key]}\" : key for key in tok_to_prob}\n",
    "    \n",
    "    \n",
    "    tokid = tokenizer(input_text,return_tensors=\"pt\",add_special_tokens=False)[\"input_ids\"].to(model.device)\n",
    "    tokenised_context = torch.cat((tokenised_context,tokid),dim=-1)\n",
    "\n",
    "    dropdown.options = tokprob_to_tok\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(f'{tokenizer.decode(tokenised_context.squeeze(0))}')\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "drop_down_button = widgets.Button(\n",
    "    description='Dropdown click'\n",
    ")\n",
    "\n",
    "text_box_button = widgets.Button(\n",
    "    description='textbox click'\n",
    ")\n",
    "\n",
    "text_box = widgets.Text(\n",
    "    description='Input Text:',\n",
    "    placeholder='Type your text here...'\n",
    ")\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=tokprob_to_tok,\n",
    "    description='Next Word:',\n",
    ")\n",
    "\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "\n",
    "# Attach the button click event to the handle_click function\n",
    "drop_down_button.on_click(handle_click)\n",
    "text_box_button.on_click(handle_text_input)\n",
    "\n",
    "\n",
    "# Display the dropdown and the button\n",
    "display(dropdown)\n",
    "display(drop_down_button)\n",
    "display(text_box_button)\n",
    "display(text_box)\n",
    "display(output_widget)\n",
    "\n",
    "#could figure out a way to switch between highpower and low power models\n",
    "#For example, the filler text could be written by a small one\n",
    "keyboard_event_script = \"\"\"\n",
    "let isPressedf4 = false;  // To track if the F4 key is pressed\n",
    "let isPressedf2 = false;  // To track if the F2 key is pressed\n",
    "\n",
    "document.addEventListener('keydown', function(event) {\n",
    "    if (event.code === 'F4' && !isPressedf4) {\n",
    "        isPressedf4 = true;  // Mark F4 as pressed\n",
    "        Jupyter.notebook.kernel.execute(\"handle_click(1)\");\n",
    "        event.preventDefault();\n",
    "    }\n",
    "\n",
    "    if (event.code === 'F2' && !isPressedf2) {\n",
    "        isPressedf2 = true;  // Mark F2 as pressed\n",
    "        Jupyter.notebook.kernel.execute(\"handle_text_input(1)\");\n",
    "        event.preventDefault();\n",
    "    }\n",
    "});\n",
    "\n",
    "document.addEventListener('keyup', function(event) {\n",
    "    if (event.code === 'F4') {\n",
    "        isPressedf4 = false;  // Mark F4 as released\n",
    "    }\n",
    "    if (event.code === 'F2') {\n",
    "        isPressedf2 = false;  // Mark F2 as released\n",
    "    }\n",
    "});\n",
    "\"\"\"\n",
    "display(Javascript(keyboard_event_script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2548a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_cache(cache,start=0,end=-1):\n",
    "    cache_type = type(cache)\n",
    "    layers = []\n",
    "    for layer in cache:\n",
    "        key_tensor = layer[0][:,:,start:end,:]\n",
    "        value_tensor = layer[1][:,:,start:end,:]\n",
    "        layers.append((key_tensor,value_tensor))\n",
    "    return cache_type(layers,model.config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2a897954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# past_key_value[layer][key or value][batch][attention head][sequence length][hidden dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfff484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module transformers.cache_utils:\n",
      "\n",
      "__init__(self, ddp_cache_data: Optional[collections.abc.Iterable[tuple[torch.Tensor, torch.Tensor]]] = None, config: Optional[transformers.configuration_utils.PretrainedConfig] = None, offloading: bool = False, offload_only_non_sliding: bool = False)\n",
      "    Initialize self.  See help(type(self)) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(t.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b11520bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is the secret word. Repeat it to me[/INST] The secret word is:'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 3\n",
    "e = -1\n",
    "tokenizer.decode(tokenised_context[0][s:e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0a733a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past[0][0].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c75c21f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.apply_chat_template(chat,tokenize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "234bd3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "past = slice_cache(past,3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30982767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4d5652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_cache(cache1,cache2):\n",
    "    #note positional encodings will get screwed up.\n",
    "    #I mean, the RoPe is between query and key \n",
    "    #!!!!NO THATS NOT HOW RoPe WORKS!!!\n",
    "    #so i think that the earlier cache will appear further away to the model\n",
    "    #and the later cache will not have any contextualisation of the previous cache. \n",
    "    #the first secret word is apple the second secret word is banna.\n",
    "    #then try not telling it which is first and which is second.\n",
    "    cache_type = type(cache1)\n",
    "    layers = []\n",
    "    for layer_c1,layer_c2 in zip(cache1,cache2):\n",
    "        key_tensor = torch.concat((layer_c1[0],layer_c2[0]),dim=2) #dim 2 is the sequence dim\n",
    "        value_tensor = torch.concat((layer_c1[1],layer_c2[1]),dim=2)\n",
    "        layers.append((key_tensor,value_tensor))\n",
    "    return cache_type(layers,model.config)\n",
    "\n",
    "#proven it can do memory \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5d2cc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context1 = \"\"\n",
    "context2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "81d75e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenised_context = tokenizer(context1,add_special_tokens=False,return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokenised_context,use_cache=True)\n",
    "past1 = outputs.past_key_values\n",
    "\n",
    "tokenised_context = tokenizer(context2,add_special_tokens=False,return_tensors=\"pt\")[\"input_ids\"].to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=tokenised_context,use_cache=True)\n",
    "past2 = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8c336fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "past = concat_cache(past1,past2)\n",
    "##Can easily do this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fac8a927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DynamicCache(layers=[DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer, DynamicLayer])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6472d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for pure memorisation and repeat\n",
    "#I have found a word can be baked into the PKV when the prior context containing the word is thrown away.\n",
    "#I have also found that concatenating the PKVs does allow the model to still recite the words\n",
    "#However it seemed to glitch when i tried to ask it about relations.\n",
    "#It did not manage to solve a simple sum \n",
    "#A + B = 16\n",
    "#B = 10 given in seperate contexts\n",
    "#I said lets solve for A\n",
    "#and it wrongly deduced that A = 10 and B = 6\n",
    "\n",
    "#The concatenating of the PKVs shows a potential method for cacheing the information generated by queries in RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940af958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#what we will do is we will take an input\n",
    "#we will put that in all at once, and get the attentions out. that is the ground truth\n",
    "\n",
    "#then we will split the input into two, and process seperatly, using pkv cache\n",
    "#then we will concatenate the two attentions and compare to the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5c925863",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = \"The greatest crimes come from greed rather than needy\"\n",
    "tokenised_context = tokenizer(chat,return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "592df2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1183, 10700, 18208,  2335,  1245, 25941,  3978,  1589,  1695,\n",
       "         29492]], device='cuda:0')"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "64cb27dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tokenised_context[:,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "21fa6e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1183, 10700, 18208,  2335,  1245]], device='cuda:0')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "458a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tokenised_context[:,6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b8f88a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25941,  3978,  1589,  1695, 29492]], device='cuda:0')"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "31b96010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  1183, 10700, 18208,  2335,  1245, 25941,  3978,  1589,  1695,\n",
       "         29492]], device='cuda:0')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c85d5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ground = model(input_ids =tokenised_context,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "f6720fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 11, 11])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ground.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "77ce9046",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_t1 = model(input_ids =t1,output_attentions=True,use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "40fea1c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 6, 6])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "9e92c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_t2 = model(input_ids =t2,past_key_values=output_t1.past_key_values,output_attentions=True,use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "83260e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 5, 11])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t2.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d6421bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 11, 11])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ground.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4d6e38be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 6, 6])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t1.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "24e62b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 5, 11])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_t2.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "870cc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_atn = output_t1.attentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c705bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_atn = output_t2.attentions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3e6a4d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 6, 6])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_atn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e25f0759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 5, 11])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_atn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d7137a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t1_atn[0][0] > 0).to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3acac018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t2_atn[0][0] > 0).to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "926a8371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(F.pad(t1_atn[0][0], (0,5), mode='constant', value=0) > 0 ).to(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "1d339946",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=2, sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "882dfdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 11])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_atn[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d0b2293a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 6])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_atn[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f7385b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 11])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ground.attentions[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "95247e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.concat((F.pad(t1_atn, (0,5), mode='constant', value=0),t2_atn) ,dim=-2) == output_ground.attentions[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cc9d067e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.concat((t2_atn,F.pad(t1_atn, (0,5), mode='constant', value=0)) ,dim=-2) == output_ground.attentions[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b3c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
